{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYAWxSDB8c45",
        "outputId": "fb9b6c10-6fcb-4e16-babc-0870d66ea702"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.23-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.23-py3-none-any.whl (877 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m877.6/877.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.9-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.23 ultralytics-thop-2.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j4SYPxC8r5j",
        "outputId": "2b417b68-700d-4785-e905-dfb935034d9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gTTS\n",
            "  Downloading gTTS-2.5.3-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.10/dist-packages (from gTTS) (8.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->gTTS) (2024.8.30)\n",
            "Downloading gTTS-2.5.3-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: gTTS\n",
            "Successfully installed gTTS-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJmFs3zi-jBq",
        "outputId": "f4e7a9a0-6db5-49c6-a269-eedd826a7fa4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W-ztYZJc9IOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f742d44d-885e-44cd-f127-09e0d6ab3ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib  # For model saving and loading\n",
        "import warnings\n",
        "from gtts import gTTS\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVxSd_tL9UqH",
        "outputId": "656cac05-d959-486e-ab51-4a6f3e06f8f9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jzTbJBKk-JAu"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Define names for the 17 keypoints\n",
        "KEYPOINT_NAMES = [\n",
        "    \"Nose\", \"Right Eye\", \"Left Eye\", \"Right Ear\", \"Left Ear\",\n",
        "    \"Right Shoulder\", \"Left Shoulder\", \"Right Elbow\", \"Left Elbow\",\n",
        "    \"Right Wrist\", \"Left Wrist\", \"Right Hip\", \"Left Hip\",\n",
        "    \"Right Knee\", \"Left Knee\", \"Right Ankle\", \"Left Ankle\"\n",
        "]\n",
        "\n",
        "# define key points to keep\n",
        "keypoints_to_keep = [\n",
        "    'Right Shoulder', 'Left Shoulder', 'Right Elbow', 'Left Elbow',\n",
        "    'Right Wrist', 'Left Wrist', 'Right Hip', 'Left Hip',\n",
        "    'Right Knee', 'Left Knee', 'Right Ankle', 'Left Ankle'\n",
        "]\n",
        "\n",
        "# construct the column names to be processed\n",
        "keypoint_columns = []\n",
        "for kp in keypoints_to_keep:\n",
        "    keypoint_columns.append(f'{kp}_x')\n",
        "    keypoint_columns.append(f'{kp}_y')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F84woSB4-kdM"
      },
      "outputs": [],
      "source": [
        "# Define the function that computes the new origin and coordinates normalization\n",
        "def calculate_new_origin(keypoints_data):\n",
        "    right_shoulder = keypoints_data[5][:2]\n",
        "    left_shoulder = keypoints_data[6][:2]\n",
        "    right_hip = keypoints_data[11][:2]\n",
        "    left_hip = keypoints_data[12][:2]\n",
        "\n",
        "    valid_points = [p for p in [right_shoulder, left_shoulder, right_hip, left_hip] if p[0] != 0 and p[1] != 0]\n",
        "    if len(valid_points) == 0:\n",
        "        raise ValueError(\"The key points of right shoulder, left shoulder, right hip, and left hip are missing and the new origin cannot be calculated\")\n",
        "\n",
        "    x0 = sum([p[0] for p in valid_points]) / len(valid_points)\n",
        "    y0 = sum([p[1] for p in valid_points]) / len(valid_points)\n",
        "    return x0, y0\n",
        "\n",
        "def get_min_max_of_new_coords(keypoints_data, x0, y0):\n",
        "    x_new_values = []\n",
        "    y_new_values = []\n",
        "\n",
        "    for keypoint in keypoints_data:\n",
        "        x, y, conf = keypoint\n",
        "        if conf > 0:  # Calculate only valid keypoints\n",
        "            x_new = x - x0\n",
        "            y_new = -(y - y0)  # The Y-axis grows from bottom to top\n",
        "            x_new_values.append(x_new)\n",
        "            y_new_values.append(y_new)\n",
        "\n",
        "    x_min_new, x_max_new = min(x_new_values), max(x_new_values)\n",
        "    y_min_new, y_max_new = min(y_new_values), max(y_new_values)\n",
        "\n",
        "    return x_min_new, x_max_new, y_min_new, y_max_new\n",
        "\n",
        "def parse_keypoints_with_custom_origin(results):\n",
        "    parsed_keypoints_list = []\n",
        "    for i in range(len(results)):\n",
        "        keypoints_data = results[i].keypoints.data.cpu().numpy()[0]  # get keypoints (17, 3)\n",
        "\n",
        "        # calculate the new origin coordinates\n",
        "        try:\n",
        "            x0, y0 = calculate_new_origin(keypoints_data)\n",
        "        except ValueError as e:\n",
        "            print(f\"Failed to calculate new origin for object {i}: {e}\")\n",
        "            continue  # Skip this object\n",
        "\n",
        "        # calculate the maximum and minimum values of x_new and y_new in the new coordinate system\n",
        "        x_min_new, x_max_new, y_min_new, y_max_new = get_min_max_of_new_coords(keypoints_data, x0, y0)\n",
        "\n",
        "        object_keypoints = {\"object_id\": i}\n",
        "        keypoint_dict = {}\n",
        "\n",
        "        # Calculate coordinates of each keypoint relative to the new origin and normalize\n",
        "        for j, keypoint in enumerate(keypoints_data):\n",
        "            x, y, conf = keypoint\n",
        "            if x == 0 and y == 0:\n",
        "                keypoint_dict[KEYPOINT_NAMES[j]] = None\n",
        "            else:\n",
        "                # Coordinates in the new coordinate system, with the Y-axis growing from bottom to top\n",
        "                x_new = x - x0\n",
        "                y_new = -(y - y0)\n",
        "\n",
        "                # normalize the new coordinates\n",
        "                if x_max_new != x_min_new:\n",
        "                    x_normalized = (x_new - x_min_new) / (x_max_new - x_min_new)\n",
        "                else:\n",
        "                    x_normalized = 0  # Avoid division by zero errors if maximum is equal to minimum\n",
        "\n",
        "                if y_max_new != y_min_new:\n",
        "                    y_normalized = (y_new - y_min_new) / (y_max_new - y_min_new)\n",
        "                else:\n",
        "                    y_normalized = 0  # Avoid division by zero errors if maximum is equal to minimum\n",
        "\n",
        "                keypoint_dict[KEYPOINT_NAMES[j]] = {\n",
        "                    \"name\": KEYPOINT_NAMES[j],\n",
        "                    \"x\": x_normalized,\n",
        "                    \"y\": y_normalized,\n",
        "                    \"confidence\": conf\n",
        "                }\n",
        "\n",
        "        object_keypoints[\"keypoints\"] = keypoint_dict\n",
        "        parsed_keypoints_list.append(object_keypoints)\n",
        "\n",
        "    return parsed_keypoints_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9Ltk9F20-znN"
      },
      "outputs": [],
      "source": [
        "# Load the model and encoder with Google Drive paths\n",
        "knn_model = joblib.load('/content/drive/MyDrive/fitness/action_classification_model.pkl')\n",
        "rf_model_sequence = joblib.load('/content/drive/MyDrive/fitness/action_stage_model.pkl')\n",
        "label_encoder = joblib.load('/content/drive/MyDrive/fitness/action_label_encoder.pkl')\n",
        "onehot_encoder = joblib.load('/content/drive/MyDrive/fitness/action_onehot_encoder.pkl')\n",
        "scaler = joblib.load('/content/drive/MyDrive/fitness/feature_scaler.pkl')\n",
        "\n",
        "# Load keypoint averages for standard actions\n",
        "standard_keypoints_mean = pd.read_pickle('/content/drive/MyDrive/fitness/standard_keypoints_mean.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VmfsDIVI6Wpc"
      },
      "outputs": [],
      "source": [
        "# Text-to-Speech\n",
        "def text_to_speech(text, filename=\"output.mp3\"):\n",
        "\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "\n",
        "    tts.save(filename)\n",
        "    print(f\"MP3 file saved as {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ogxoELuZR93c"
      },
      "outputs": [],
      "source": [
        "# Define a function to process a new video and output error types\n",
        "def process_new_video(input_video_path):\n",
        "    # Load YOLOv8 pose model\n",
        "    model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "    # Extract keypoint data from the video\n",
        "    all_data = []\n",
        "\n",
        "    # Output the currently processing video file\n",
        "    print(f\"Processing video: {input_video_path}\")\n",
        "    # Set default action name to 'Unknown'\n",
        "    action_name = 'Unknown'\n",
        "    standard_type = 'nonstandard'  # Default to nonstandard action\n",
        "\n",
        "    # Open the video file\n",
        "    cap = cv2.VideoCapture(input_video_path)  # Use OpenCV to open the video file.\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)  # Get the frame rate (FPS) of the video.\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Get the total number of frames.\n",
        "    duration = frame_count / fps  # Calculate the total duration of the video.\n",
        "\n",
        "    # Take 5 frames every 0.5 seconds\n",
        "    frames_per_half_second = int(fps / 2)\n",
        "    max_duration = 3.0  # Only process the first 3 seconds of the video\n",
        "    max_frame_to_process = int(fps * max_duration)\n",
        "\n",
        "    frame_indices = []\n",
        "    for i in range(0, max_frame_to_process, frames_per_half_second):\n",
        "        # Take 3 frames at intervals from 5 frames\n",
        "        for j in [0, 2, 4]:  # Select the 1st, 3rd, and 5th frames\n",
        "            frame_index = i + j\n",
        "            if frame_index < max_frame_to_process:\n",
        "                frame_indices.append(frame_index)\n",
        "\n",
        "    frame_indices = sorted(set(frame_indices))  # Ensure no duplicate frames, ordered\n",
        "\n",
        "    # Process the video frame by frame\n",
        "    frame_number = 0\n",
        "    sequence_number = 0\n",
        "\n",
        "    while cap.isOpened():  # Process each frame of the video in a loop\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_number in frame_indices:  # Only process specified frames (in frame_indices).\n",
        "            # Perform detection using the model\n",
        "            results = model(frame)\n",
        "            parsed_keypoints = parse_keypoints_with_custom_origin(results)\n",
        "\n",
        "            # Assign the same sequence number for frames within 0.5 seconds\n",
        "            sequence_number = frame_number // frames_per_half_second\n",
        "\n",
        "            for obj in parsed_keypoints:\n",
        "                keypoints = obj['keypoints']\n",
        "                row_data = {\n",
        "                    'action_name': action_name,\n",
        "                    'standard_type': standard_type,\n",
        "                    'frame_index': frame_number,\n",
        "                    'sequence': sequence_number,\n",
        "                }\n",
        "                # Store each keypoint's coordinates in row_data\n",
        "                for kp_name in KEYPOINT_NAMES:\n",
        "                    kp_info = keypoints.get(kp_name, None)\n",
        "                    if kp_info:\n",
        "                        row_data[f'{kp_name}_x'] = kp_info['x']\n",
        "                        row_data[f'{kp_name}_y'] = kp_info['y']\n",
        "                    else:\n",
        "                        row_data[f'{kp_name}_x'] = None\n",
        "                        row_data[f'{kp_name}_y'] = None\n",
        "\n",
        "                all_data.append(row_data)\n",
        "\n",
        "        frame_number += 1\n",
        "\n",
        "    # Release video resources\n",
        "    cap.release()\n",
        "\n",
        "    # Convert all data to DataFrame\n",
        "    columns = ['action_name', 'standard_type', 'frame_index', 'sequence'] + [f'{k}_x' for k in KEYPOINT_NAMES] + [f'{k}_y' for k in KEYPOINT_NAMES]\n",
        "    data = pd.DataFrame(all_data, columns=columns)\n",
        "\n",
        "    # Data cleaning and mean imputation\n",
        "    data[keypoint_columns] = data[keypoint_columns].fillna(data[keypoint_columns].mean())\n",
        "\n",
        "    # Prepare features and labels\n",
        "    features = keypoint_columns\n",
        "    X = data[features]\n",
        "\n",
        "    # Use the action classification model for prediction\n",
        "    y_pred_action_encoded = knn_model.predict(X)\n",
        "    y_pred_action = label_encoder.inverse_transform(y_pred_action_encoded)\n",
        "    data['predicted_action'] = y_pred_action\n",
        "\n",
        "    # Update the action name\n",
        "    action_name = y_pred_action[0]  # Assume the action name is the same for all frames\n",
        "    data['action_name'] = action_name\n",
        "\n",
        "    # Default to nonstandard action\n",
        "    data['standard_type'] = 'nonstandard'\n",
        "\n",
        "    # Prepare features for the action phase classification model\n",
        "    # Prepare three-frame combination features for nonstandard actions and make predictions\n",
        "    grouped_data_nonstandard = []\n",
        "    group_size = 3  # Group every three frames\n",
        "\n",
        "    for name, group in data.groupby(['action_name', 'standard_type', 'sequence']):\n",
        "        if len(group) >= group_size:\n",
        "            for i in range(0, len(group) - group_size + 1, 1):\n",
        "                frames = group.iloc[i:i + group_size]\n",
        "\n",
        "                second_frame = frames.iloc[1][keypoint_columns].values\n",
        "                first_diff = frames.iloc[0][keypoint_columns].values - frames.iloc[1][keypoint_columns].values\n",
        "                third_diff = frames.iloc[2][keypoint_columns].values - frames.iloc[1][keypoint_columns].values\n",
        "\n",
        "                combined_features = np.hstack([second_frame, first_diff, third_diff])\n",
        "\n",
        "                grouped_data_nonstandard.append({\n",
        "                    'action_name': name[0],\n",
        "                    'standard_type': name[1],\n",
        "                    'sequence': name[2],\n",
        "                    'features': combined_features,\n",
        "                    'second_frame_keypoints': second_frame  # Save middle frame keypoints\n",
        "                })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    grouped_df_nonstandard = pd.DataFrame(grouped_data_nonstandard)\n",
        "\n",
        "    if grouped_df_nonstandard.empty:\n",
        "        print(\"Unable to extract valid three-frame combination features from the video. The video may be too short or keypoint detection may have failed.\")\n",
        "        return\n",
        "\n",
        "    # Perform one-hot encoding for action_name (using previously trained encoder)\n",
        "    action_name_encoded_nonstandard = onehot_encoder.transform(grouped_df_nonstandard[['action_name']])\n",
        "\n",
        "    # Combine the one-hot encoded categorical features with the numerical features\n",
        "    X_nonstandard = np.hstack((action_name_encoded_nonstandard, np.vstack(grouped_df_nonstandard['features'].values)))\n",
        "\n",
        "    # Standardize the features (using previously trained scaler)\n",
        "    X_nonstandard_scaled = scaler.transform(X_nonstandard)\n",
        "\n",
        "    # Use the trained model for prediction\n",
        "    y_pred_nonstandard = rf_model_sequence.predict(X_nonstandard_scaled)\n",
        "\n",
        "    # Add prediction results to DataFrame\n",
        "    grouped_df_nonstandard['predicted_sequence'] = y_pred_nonstandard\n",
        "\n",
        "    # 9. Compare nonstandard and standard action keypoints to identify error types\n",
        "\n",
        "    # Define error types mapping\n",
        "    error_types = []\n",
        "\n",
        "    # Define the indices for each keypoint\n",
        "    keypoint_indices = {keypoint: idx for idx, keypoint in enumerate(keypoint_columns)}\n",
        "\n",
        "    # Define a function to calculate joint angles\n",
        "    def calculate_angle(a, b, c):\n",
        "        \"\"\"\n",
        "        Calculate the angle formed by three points: a, b, c.\n",
        "        Return the angle in degrees.\n",
        "        \"\"\"\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        # Prevent numerical errors that could result in values outside the [-1, 1] range\n",
        "        cosine_angle = np.clip(cosine_angle, -1.0, 1.0)\n",
        "        angle = np.arccos(cosine_angle)\n",
        "        return np.degrees(angle)\n",
        "\n",
        "    for idx, row in grouped_df_nonstandard.iterrows():\n",
        "        action_name = row['action_name']\n",
        "        predicted_sequence = row['predicted_sequence']\n",
        "        nonstandard_keypoints = row['second_frame_keypoints']\n",
        "\n",
        "        # Get the average keypoints for the corresponding standard action\n",
        "        standard_row = standard_keypoints_mean[\n",
        "            (standard_keypoints_mean['action_name'] == action_name) &\n",
        "            (standard_keypoints_mean['sequence'] == predicted_sequence)\n",
        "        ]\n",
        "\n",
        "        if standard_row.empty:\n",
        "            # If no matching standard action, skip\n",
        "            error_types.append('Unable to match standard action')\n",
        "            continue\n",
        "\n",
        "        standard_keypoints = standard_row.iloc[0]['second_frame_keypoints']\n",
        "\n",
        "        # Calculate keypoint differences\n",
        "        differences = nonstandard_keypoints - standard_keypoints  # Difference: nonstandard - standard\n",
        "\n",
        "        # Initialize error type as None\n",
        "        error_type = None\n",
        "\n",
        "        # 1. Check x-direction differences for left and right wrists\n",
        "        left_wrist_x_diff = differences[keypoint_indices['Left Wrist_x']]\n",
        "        right_wrist_x_diff = differences[keypoint_indices['Right Wrist_x']]\n",
        "\n",
        "        # Set a threshold to determine if the deviation is too large (adjust based on data distribution)\n",
        "        threshold_hand = 0.1  # For example, if the deviation exceeds 0.1\n",
        "\n",
        "#  error label:1.Left hand too far left\n",
        "#               2.Left hand too far right\n",
        "#               3.Right hand too far left\n",
        "#               4.Right hand too far right\n",
        "#               5.Left arm not bent enough\n",
        "#               6.Left arm too bent\n",
        "#               7.Right arm not bent enough\n",
        "#               8.Right arm too bent\n",
        "#               9.Left leg not bent enough\n",
        "#               10.Left leg too bent\n",
        "#               11.Right leg not bent enough\n",
        "#               12.Right leg too bent\n",
        "#               13.Waist misaligned\n",
        "#               14.No significant error detected\n",
        "        if abs(left_wrist_x_diff) > threshold_hand:\n",
        "            if left_wrist_x_diff < 0:\n",
        "                error_type = 'Left hand too far left'\n",
        "            else:\n",
        "                error_type = 'Left hand too far right'\n",
        "        elif abs(right_wrist_x_diff) > threshold_hand:\n",
        "            if right_wrist_x_diff < 0:\n",
        "                error_type = 'Right hand too far left'\n",
        "            else:\n",
        "                error_type = 'Right hand too far right'\n",
        "        else:\n",
        "            # 2. Check left/right elbow angles\n",
        "            # Get nonstandard and standard keypoint coordinates\n",
        "            left_shoulder = nonstandard_keypoints[keypoint_indices['Left Shoulder_x']:keypoint_indices['Left Shoulder_y']+1]\n",
        "            left_elbow = nonstandard_keypoints[keypoint_indices['Left Elbow_x']:keypoint_indices['Left Elbow_y']+1]\n",
        "            left_wrist = nonstandard_keypoints[keypoint_indices['Left Wrist_x']:keypoint_indices['Left Wrist_y']+1]\n",
        "\n",
        "            right_shoulder = nonstandard_keypoints[keypoint_indices['Right Shoulder_x']:keypoint_indices['Right Shoulder_y']+1]\n",
        "            right_elbow = nonstandard_keypoints[keypoint_indices['Right Elbow_x']:keypoint_indices['Right Elbow_y']+1]\n",
        "            right_wrist = nonstandard_keypoints[keypoint_indices['Right Wrist_x']:keypoint_indices['Right Wrist_y']+1]\n",
        "\n",
        "            # Standard action keypoints\n",
        "            left_shoulder_std = standard_keypoints[keypoint_indices['Left Shoulder_x']:keypoint_indices['Left Shoulder_y']+1]\n",
        "            left_elbow_std = standard_keypoints[keypoint_indices['Left Elbow_x']:keypoint_indices['Left Elbow_y']+1]\n",
        "            left_wrist_std = standard_keypoints[keypoint_indices['Left Wrist_x']:keypoint_indices['Left Wrist_y']+1]\n",
        "\n",
        "            right_shoulder_std = standard_keypoints[keypoint_indices['Right Shoulder_x']:keypoint_indices['Right Shoulder_y']+1]\n",
        "            right_elbow_std = standard_keypoints[keypoint_indices['Right Elbow_x']:keypoint_indices['Right Elbow_y']+1]\n",
        "            right_wrist_std = standard_keypoints[keypoint_indices['Right Wrist_x']:keypoint_indices['Right Wrist_y']+1]\n",
        "\n",
        "            # Calculate angles\n",
        "            left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
        "            left_elbow_angle_std = calculate_angle(left_shoulder_std, left_elbow_std, left_wrist_std)\n",
        "\n",
        "            right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
        "            right_elbow_angle_std = calculate_angle(right_shoulder_std, right_elbow_std, right_wrist_std)\n",
        "\n",
        "            # Calculate angle differences\n",
        "            left_elbow_angle_diff = left_elbow_angle - left_elbow_angle_std\n",
        "            right_elbow_angle_diff = right_elbow_angle - right_elbow_angle_std\n",
        "\n",
        "            # Set angle difference threshold (adjust as needed)\n",
        "            threshold_angle = 10  # Angle difference exceeds 10 degrees\n",
        "\n",
        "            if abs(left_elbow_angle_diff) > threshold_angle:\n",
        "                if left_elbow_angle_diff > 0:\n",
        "                    error_type = 'Left arm not bent enough'\n",
        "                else:\n",
        "                    error_type = 'Left arm too bent'\n",
        "            elif abs(right_elbow_angle_diff) > threshold_angle:\n",
        "                if right_elbow_angle_diff > 0:\n",
        "                    error_type = 'Right arm not bent enough'\n",
        "                else:\n",
        "                    error_type = 'Right arm too bent'\n",
        "            else:\n",
        "                # 3. Check left/right knee angles\n",
        "                left_hip = nonstandard_keypoints[keypoint_indices['Left Hip_x']:keypoint_indices['Left Hip_y']+1]\n",
        "                left_knee = nonstandard_keypoints[keypoint_indices['Left Knee_x']:keypoint_indices['Left Knee_y']+1]\n",
        "                left_ankle = nonstandard_keypoints[keypoint_indices['Left Ankle_x']:keypoint_indices['Left Ankle_y']+1]\n",
        "\n",
        "                right_hip = nonstandard_keypoints[keypoint_indices['Right Hip_x']:keypoint_indices['Right Hip_y']+1]\n",
        "                right_knee = nonstandard_keypoints[keypoint_indices['Right Knee_x']:keypoint_indices['Right Knee_y']+1]\n",
        "                right_ankle = nonstandard_keypoints[keypoint_indices['Right Ankle_x']:keypoint_indices['Right Ankle_y']+1]\n",
        "\n",
        "                left_hip_std = standard_keypoints[keypoint_indices['Left Hip_x']:keypoint_indices['Left Hip_y']+1]\n",
        "                left_knee_std = standard_keypoints[keypoint_indices['Left Knee_x']:keypoint_indices['Left Knee_y']+1]\n",
        "                left_ankle_std = standard_keypoints[keypoint_indices['Left Ankle_x']:keypoint_indices['Left Ankle_y']+1]\n",
        "\n",
        "                right_hip_std = standard_keypoints[keypoint_indices['Right Hip_x']:keypoint_indices['Right Hip_y']+1]\n",
        "                right_knee_std = standard_keypoints[keypoint_indices['Right Knee_x']:keypoint_indices['Right Knee_y']+1]\n",
        "                right_ankle_std = standard_keypoints[keypoint_indices['Right Ankle_x']:keypoint_indices['Right Ankle_y']+1]\n",
        "\n",
        "                # Calculate angles\n",
        "                left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)\n",
        "                left_knee_angle_std = calculate_angle(left_hip_std, left_knee_std, left_ankle_std)\n",
        "\n",
        "                right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)\n",
        "                right_knee_angle_std = calculate_angle(right_hip_std, right_knee_std, right_ankle_std)\n",
        "\n",
        "                # Calculate angle differences\n",
        "                left_knee_angle_diff = left_knee_angle - left_knee_angle_std\n",
        "                right_knee_angle_diff = right_knee_angle - right_knee_angle_std\n",
        "\n",
        "                if abs(left_knee_angle_diff) > threshold_angle:\n",
        "                    if left_knee_angle_diff > 0:\n",
        "                        error_type = 'Left leg not bent enough'\n",
        "                    else:\n",
        "                        error_type = 'Left leg too bent'\n",
        "                elif abs(right_knee_angle_diff) > threshold_angle:\n",
        "                    if right_knee_angle_diff > 0:\n",
        "                        error_type = 'Right leg not bent enough'\n",
        "                    else:\n",
        "                        error_type = 'Right leg too bent'\n",
        "                else:\n",
        "                    # 4. Check if the waist is bent (by comparing the horizontal positions of the left/right shoulders and hips)\n",
        "                    left_shoulder = nonstandard_keypoints[keypoint_indices['Left Shoulder_x']:keypoint_indices['Left Shoulder_y']+1]\n",
        "                    right_shoulder = nonstandard_keypoints[keypoint_indices['Right Shoulder_x']:keypoint_indices['Right Shoulder_y']+1]\n",
        "                    left_hip = nonstandard_keypoints[keypoint_indices['Left Hip_x']:keypoint_indices['Left Hip_y']+1]\n",
        "                    right_hip = nonstandard_keypoints[keypoint_indices['Right Hip_x']:keypoint_indices['Right Hip_y']+1]\n",
        "\n",
        "                    # Calculate the midpoint of shoulders and hips\n",
        "                    shoulders_midpoint = (left_shoulder + right_shoulder) / 2\n",
        "                    hips_midpoint = (left_hip + right_hip) / 2\n",
        "\n",
        "                    # Calculate vertical offset\n",
        "                    vertical_diff = abs(shoulders_midpoint[0] - hips_midpoint[0])  # x-direction difference\n",
        "\n",
        "                    # Set threshold\n",
        "                    threshold_waist = 0.05  # Adjust as needed\n",
        "\n",
        "                    if vertical_diff > threshold_waist:\n",
        "                        error_type = 'Waist misaligned'\n",
        "                    else:\n",
        "                        error_type = 'No significant error detected'\n",
        "\n",
        "        error_types.append(error_type)\n",
        "\n",
        "    # Add error types to DataFrame\n",
        "    grouped_df_nonstandard['error_type'] = error_types\n",
        "\n",
        "    # Output results\n",
        "    print(grouped_df_nonstandard[['action_name', 'sequence', 'predicted_sequence', 'error_type']])\n",
        "\n",
        "    # os mkdir\n",
        "    os.makedirs(\"mp3\", exist_ok=True)\n",
        "\n",
        "    # Text-to-Speech\n",
        "    for row in grouped_df_nonstandard.itertuples():\n",
        "        text_to_speech(row.error_type, f\"mp3/output_sequence_{row.action_name}_{row.sequence + 1}_{row.error_type}.mp3\")\n",
        "\n",
        "    # Save results to Excel\n",
        "    grouped_df_nonstandard.to_excel(\"test_keypoints_error.xlsx\", index=False)\n",
        "    print(\"Error type results have been saved to test_keypoints_error.xlsx\")\n",
        "\n",
        "    return error_types\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Zl0oGWAM6Wpf"
      },
      "outputs": [],
      "source": [
        "def process_video_with_text(input_video_path, output_video_path, text_list):\n",
        "    # Load model\n",
        "    model = YOLO('yolov8n-pose.pt')\n",
        "\n",
        "    # Open video file\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calculate the frame interval for each text to appear\n",
        "    if len(text_list) > 0:\n",
        "        interval = total_frames // len(text_list)\n",
        "    else:\n",
        "        interval = total_frames  # Prevent division by zero\n",
        "\n",
        "    # Create VideoWriter object to save the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "    # Set font, font size, and color\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2\n",
        "    font_color = (0, 0, 255)  # Red text (BGR format)\n",
        "    thickness = 2  # Thicker line for bold text\n",
        "    position = (15, 60)  # Position of the text in the top-left corner\n",
        "\n",
        "    # Process the video frame by frame\n",
        "    frame_count = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Perform detection using the model\n",
        "        results = model(frame)\n",
        "\n",
        "        # Add text to the frame\n",
        "        text_index = frame_count // interval\n",
        "        if text_index < len(text_list):\n",
        "            text = text_list[text_index]\n",
        "            # Display the text in the top-left corner of the frame with bold red font\n",
        "            cv2.putText(frame, text, position, font, font_scale, font_color, thickness, cv2.LINE_AA)\n",
        "\n",
        "        # Draw the model detection results\n",
        "        for r in results:\n",
        "            im_array = r.plot()  # Directly draw the image\n",
        "            # Write the processed frame to the output video\n",
        "            out.write(im_array)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mIyVFKaxT1BT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e157b03e-eff2-48f2-8770-c84affe090a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video: /content/drive/My Drive/fitness/dataset_fitness/02 Barbell Bench Press/nonstandard04.mov\n",
            "\n",
            "0: 640x480 1 person, 168.0ms\n",
            "Speed: 6.7ms preprocess, 168.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.2ms\n",
            "Speed: 5.2ms preprocess, 164.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.4ms\n",
            "Speed: 5.4ms preprocess, 160.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 185.2ms\n",
            "Speed: 4.7ms preprocess, 185.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.0ms\n",
            "Speed: 4.6ms preprocess, 164.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 155.4ms\n",
            "Speed: 4.7ms preprocess, 155.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 171.1ms\n",
            "Speed: 5.6ms preprocess, 171.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 166.3ms\n",
            "Speed: 5.6ms preprocess, 166.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 161.6ms\n",
            "Speed: 5.4ms preprocess, 161.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 156.1ms\n",
            "Speed: 4.1ms preprocess, 156.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 167.7ms\n",
            "Speed: 4.2ms preprocess, 167.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.8ms\n",
            "Speed: 5.5ms preprocess, 160.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 157.7ms\n",
            "Speed: 5.1ms preprocess, 157.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 171.2ms\n",
            "Speed: 5.1ms preprocess, 171.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 166.9ms\n",
            "Speed: 5.7ms preprocess, 166.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 158.9ms\n",
            "Speed: 5.7ms preprocess, 158.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 158.2ms\n",
            "Speed: 5.9ms preprocess, 158.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.0ms\n",
            "Speed: 5.0ms preprocess, 164.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "           action_name  sequence  predicted_sequence               error_type\n",
            "0  Barbell Bench Press         0                   1  Left hand too far right\n",
            "1  Barbell Bench Press         1                   2  Left hand too far right\n",
            "2  Barbell Bench Press         2                   4  Left hand too far right\n",
            "3  Barbell Bench Press         3                   3  Left hand too far right\n",
            "4  Barbell Bench Press         4                   5  Left hand too far right\n",
            "5  Barbell Bench Press         5                   2  Left hand too far right\n",
            "MP3 file saved as mp3/output_sequence_Barbell Bench Press_1_Left hand too far right.mp3\n",
            "MP3 file saved as mp3/output_sequence_Barbell Bench Press_2_Left hand too far right.mp3\n",
            "MP3 file saved as mp3/output_sequence_Barbell Bench Press_3_Left hand too far right.mp3\n",
            "MP3 file saved as mp3/output_sequence_Barbell Bench Press_4_Left hand too far right.mp3\n",
            "MP3 file saved as mp3/output_sequence_Barbell Bench Press_5_Left hand too far right.mp3\n",
            "MP3 file saved as mp3/output_sequence_Barbell Bench Press_6_Left hand too far right.mp3\n",
            "Error type results have been saved to test_keypoints_error.xlsx\n",
            "\n",
            "0: 640x480 1 person, 175.5ms\n",
            "Speed: 5.8ms preprocess, 175.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 171.0ms\n",
            "Speed: 5.3ms preprocess, 171.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 169.7ms\n",
            "Speed: 5.0ms preprocess, 169.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 210.5ms\n",
            "Speed: 5.3ms preprocess, 210.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 275.1ms\n",
            "Speed: 7.4ms preprocess, 275.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 256.2ms\n",
            "Speed: 5.0ms preprocess, 256.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 255.9ms\n",
            "Speed: 10.0ms preprocess, 255.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 287.4ms\n",
            "Speed: 7.3ms preprocess, 287.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 262.1ms\n",
            "Speed: 5.8ms preprocess, 262.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 267.1ms\n",
            "Speed: 12.4ms preprocess, 267.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 264.0ms\n",
            "Speed: 5.2ms preprocess, 264.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 282.9ms\n",
            "Speed: 8.2ms preprocess, 282.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 271.0ms\n",
            "Speed: 5.3ms preprocess, 271.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 286.7ms\n",
            "Speed: 7.4ms preprocess, 286.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 293.9ms\n",
            "Speed: 8.0ms preprocess, 293.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 286.1ms\n",
            "Speed: 5.9ms preprocess, 286.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 267.9ms\n",
            "Speed: 6.4ms preprocess, 267.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 244.7ms\n",
            "Speed: 15.2ms preprocess, 244.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 168.2ms\n",
            "Speed: 5.0ms preprocess, 168.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 177.6ms\n",
            "Speed: 5.0ms preprocess, 177.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.0ms\n",
            "Speed: 10.5ms preprocess, 160.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 165.0ms\n",
            "Speed: 10.4ms preprocess, 165.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 163.7ms\n",
            "Speed: 5.0ms preprocess, 163.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.7ms\n",
            "Speed: 5.0ms preprocess, 164.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.2ms\n",
            "Speed: 4.8ms preprocess, 164.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 171.5ms\n",
            "Speed: 7.3ms preprocess, 171.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 169.8ms\n",
            "Speed: 6.2ms preprocess, 169.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 156.9ms\n",
            "Speed: 5.0ms preprocess, 156.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 162.2ms\n",
            "Speed: 5.5ms preprocess, 162.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 176.0ms\n",
            "Speed: 5.2ms preprocess, 176.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 161.8ms\n",
            "Speed: 5.1ms preprocess, 161.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 154.8ms\n",
            "Speed: 5.2ms preprocess, 154.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 172.6ms\n",
            "Speed: 5.1ms preprocess, 172.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 171.3ms\n",
            "Speed: 5.0ms preprocess, 171.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 162.1ms\n",
            "Speed: 7.9ms preprocess, 162.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 161.0ms\n",
            "Speed: 5.1ms preprocess, 161.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 169.9ms\n",
            "Speed: 5.0ms preprocess, 169.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 162.3ms\n",
            "Speed: 5.2ms preprocess, 162.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 183.6ms\n",
            "Speed: 7.0ms preprocess, 183.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 161.9ms\n",
            "Speed: 5.6ms preprocess, 161.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.8ms\n",
            "Speed: 5.0ms preprocess, 160.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 172.2ms\n",
            "Speed: 9.0ms preprocess, 172.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 172.9ms\n",
            "Speed: 5.2ms preprocess, 172.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 157.1ms\n",
            "Speed: 5.1ms preprocess, 157.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 167.1ms\n",
            "Speed: 5.0ms preprocess, 167.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 172.7ms\n",
            "Speed: 5.1ms preprocess, 172.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 189.4ms\n",
            "Speed: 5.1ms preprocess, 189.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.4ms\n",
            "Speed: 5.8ms preprocess, 160.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 168.1ms\n",
            "Speed: 6.2ms preprocess, 168.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 162.6ms\n",
            "Speed: 5.1ms preprocess, 162.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 206.0ms\n",
            "Speed: 4.8ms preprocess, 206.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 166.0ms\n",
            "Speed: 4.7ms preprocess, 166.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.7ms\n",
            "Speed: 6.0ms preprocess, 164.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 158.6ms\n",
            "Speed: 6.3ms preprocess, 158.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 203.8ms\n",
            "Speed: 4.9ms preprocess, 203.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 159.3ms\n",
            "Speed: 5.2ms preprocess, 159.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 170.5ms\n",
            "Speed: 6.2ms preprocess, 170.5ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.9ms\n",
            "Speed: 6.3ms preprocess, 160.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 183.8ms\n",
            "Speed: 4.7ms preprocess, 183.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 193.3ms\n",
            "Speed: 6.5ms preprocess, 193.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 273.2ms\n",
            "Speed: 5.0ms preprocess, 273.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 275.2ms\n",
            "Speed: 14.8ms preprocess, 275.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 270.0ms\n",
            "Speed: 6.1ms preprocess, 270.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 248.9ms\n",
            "Speed: 6.1ms preprocess, 248.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 267.1ms\n",
            "Speed: 5.5ms preprocess, 267.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 249.2ms\n",
            "Speed: 5.1ms preprocess, 249.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 255.5ms\n",
            "Speed: 5.9ms preprocess, 255.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 273.1ms\n",
            "Speed: 9.0ms preprocess, 273.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 274.1ms\n",
            "Speed: 8.0ms preprocess, 274.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 261.3ms\n",
            "Speed: 5.2ms preprocess, 261.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 282.6ms\n",
            "Speed: 5.4ms preprocess, 282.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 309.1ms\n",
            "Speed: 5.0ms preprocess, 309.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 272.1ms\n",
            "Speed: 4.9ms preprocess, 272.1ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 162.8ms\n",
            "Speed: 5.9ms preprocess, 162.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 158.3ms\n",
            "Speed: 5.3ms preprocess, 158.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 167.7ms\n",
            "Speed: 5.5ms preprocess, 167.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 165.0ms\n",
            "Speed: 5.6ms preprocess, 165.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 184.1ms\n",
            "Speed: 5.2ms preprocess, 184.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 174.3ms\n",
            "Speed: 4.9ms preprocess, 174.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 160.9ms\n",
            "Speed: 7.1ms preprocess, 160.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 161.5ms\n",
            "Speed: 7.7ms preprocess, 161.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 185.6ms\n",
            "Speed: 6.8ms preprocess, 185.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 175.6ms\n",
            "Speed: 4.7ms preprocess, 175.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 166.4ms\n",
            "Speed: 4.9ms preprocess, 166.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 161.0ms\n",
            "Speed: 5.1ms preprocess, 161.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 177.2ms\n",
            "Speed: 5.1ms preprocess, 177.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 165.0ms\n",
            "Speed: 4.0ms preprocess, 165.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 163.1ms\n",
            "Speed: 4.7ms preprocess, 163.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 158.7ms\n",
            "Speed: 4.5ms preprocess, 158.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 174.0ms\n",
            "Speed: 5.3ms preprocess, 174.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 164.2ms\n",
            "Speed: 5.3ms preprocess, 164.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "0: 640x480 1 person, 166.5ms\n",
            "Speed: 5.6ms preprocess, 166.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8fc76446-036d-4a42-ae26-b8412b5d0a22\", \"output_nonstandard05.mp4\", 5014241)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Specify the path for the input and output video files\n",
        "test_video_path = '/content/drive/My Drive/fitness/dataset_fitness/02 Barbell Bench Press/nonstandard04.mov'\n",
        "output_video_path = 'output_nonstandard05.mp4'\n",
        "\n",
        "# Call the processing function to handle the new video\n",
        "error_types = process_new_video(test_video_path)\n",
        "\n",
        "# Process the output video with text annotations\n",
        "process_video_with_text(test_video_path, output_video_path, error_types)\n",
        "\n",
        "# Download the processed output video to the local system\n",
        "files.download(output_video_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all files in the mp3 folder\n",
        "mp3_files = os.listdir('mp3')\n",
        "print(mp3_files)\n",
        "\n",
        "# Compress the entire mp3 folder into a zip file\n",
        "shutil.make_archive('mp3_files', 'zip', 'mp3')\n",
        "\n",
        "# Download the compressed zip file\n",
        "files.download('mp3_files.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Pieuc3_aWEY1",
        "outputId": "464ffce0-2a75-47fe-a156-c5b3c8397c88"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['output_sequence_Barbell Bench Press_6_Left hand too far right.mp3', 'output_sequence_Barbell Bench Press_2_Left hand too far right.mp3', 'output_sequence_Barbell Bench Press_1_Left hand too far right.mp3', 'output_sequence_Barbell Bench Press_4_Left hand too far right.mp3', 'output_sequence_Barbell Bench Press_5_Left hand too far right.mp3', 'output_sequence_Barbell Bench Press_3_Left hand too far right.mp3']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d9be0d47-d261-47a8-936b-a565cc7d2437\", \"mp3_files.zip\", 91216)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}